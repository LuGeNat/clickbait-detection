{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from collections import defaultdict\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances_file_train = \"clickbait17-validation-170630/instances.jsonl\"\n",
    "truth_file_train     = \"clickbait17-validation-170630/truth.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read thr data from files\n",
    "instances = []\n",
    "truth = []\n",
    "\n",
    "with open(instances_file_train, \"r\") as inf:\n",
    "    instances = [json.loads(x) for x in inf.readlines()]\n",
    "with open(truth_file_train, \"r\") as inf:\n",
    "    truth = [json.loads(x) for x in inf.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compact relevant data into one list of dicts\n",
    "dataset = {}\n",
    "\n",
    "# lists: postText, targetParagraphs, targetCaptions\n",
    "for i in instances:\n",
    "    dataset[i['id']] = {'postText': i['postText'], 'targetTitle': i['targetTitle'],\n",
    "                        'targetDescription': i['targetDescription'], 'targetKeywords': i['targetKeywords'], \n",
    "                        'targetParagraphs': i['targetParagraphs'], 'targetCaptions': i['targetCaptions']}\n",
    "\n",
    "for t in truth:\n",
    "    dataset[t['id']]['truthMean'] = t['truthMean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [v['targetParagraphs'] for v in dataset.values()] \n",
    "flattened = [item for sublist in texts for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for i,t in enumerate(flattened):\n",
    "    text_dict[i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([(x,) for x in text_dict.values()], ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = data                                                   \\\n",
    "    .map( lambda document: document['text'].strip().lower())            \\\n",
    "    .map( lambda document: re.split(\"[\\s;,#]\", document))       \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "termCounts = tokens                             \\\n",
    "    .flatMap(lambda document: document)         \\\n",
    "    .map(lambda word: (word, 1))                \\\n",
    "    .reduceByKey( lambda x,y: x + y)            \\\n",
    "    .map(lambda tuple: (tuple[1], tuple[0]))    \\\n",
    "    .sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index each one and collect them into a map\n",
    "vocabulary = termCounts                         \\\n",
    "    .map(lambda x: x[1])                        \\\n",
    "    .zipWithIndex()                             \\\n",
    "    .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_voc = {value: key for (key, value) in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the given document into a vector of word counts\n",
    "def document_vector(document):\n",
    "    id = document[1]\n",
    "    counts = defaultdict(int)\n",
    "    for token in document[0]:\n",
    "        if token in vocabulary:\n",
    "            token_id = vocabulary[token]\n",
    "            counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    keys = [x[0] for x in counts]\n",
    "    values = [x[1] for x in counts]\n",
    "    return (id, Vectors.sparse(len(vocabulary), keys, values))\n",
    "\n",
    "# Process all of the documents into word vectors using the \n",
    "# `document_vector` function defined previously\n",
    "documents = tokens.zipWithIndex().map(document_vector).map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an output file\n",
    "with open(\"lda_model.txt\", 'w') as f:\n",
    "    lda_model = LDA.train(documents, k=50, maxIterations=int(1e3))\n",
    "\n",
    "    topic_indices = lda_model.describeTopics(maxTermsPerTopic=int(1e3))\n",
    "        \n",
    "    # Print topics, showing the top-weighted 10 terms for each topic\n",
    "    for i in range(len(topic_indices)):\n",
    "        f.write(\"Topic #{0}\\n\".format(i + 1))\n",
    "        for j in range(len(topic_indices[i][0])):\n",
    "            f.write(\"{0}\\t{1}\\n\".format(inv_voc[topic_indices[i][0][j]] \\\n",
    "                .encode('utf-8'), topic_indices[i][1][j]))\n",
    "            \n",
    "\n",
    "    f.write(\"{0} topics distributed over {1} documents and {2} unique words\\n\"  \\\n",
    "        .format(num_topics, documents.count(), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark BETAWEB",
   "language": "python",
   "name": "pyspark-betaweb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
